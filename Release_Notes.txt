XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
obsproc_global release 1.0.0 --> released Jan ??, 2014

files
  obsproc_global/ecf/cdas/dump/jcdas_dump.ecf
  obsproc_global/ecf/gdas/dump/jgdas_dump.ecf
  obsproc_global/ecf/gfs/dump/jgfs_dump.ecf
  obsproc_global/jobs/JCDAS_DUMP
  obsproc_global/jobs/JGDAS_DUMP
  obsproc_global/jobs/JGFS_DUMP
  obsproc_global/parm/prepobs_prepssmi.cdas.parm
  obsproc_global/parm/prepobs_prepssmi.gdas.parm
  obsproc_global/parm/prepobs_prepssmi.gfs.parm
  obsproc_global/scripts/exglobal_dump.sh.ecf

( * - not changed)


 JOB script changes:
   JCDAS_DUMP, JGDAS_DUMP, JGFS_DUMP:
    - Changed all "date" commands to "date -u" since WCOSS should always
      present date in UTC.
    - Obtains obsproc_global, obsproc_dump and obsproc_shared/bufr_dumplist
      version numbers via imported environment variables $obsproc_global_ver,
      $obsproc_dump_ver and obsproc_shared_bufr_dumplist_ver, resp.
      These are defined in the upstream ecflow script.
    - Full environmental equivalence.  Streamlined, allows for more
      generalization (e.g., for checkout runs).
    - Exports new environment variable $HOMEobsproc_dump which points to
      directory path for generic dump subdirectories under version control
      (in production this is normally /nwprod/obsproc_dump.$obsproc_dump_ver).
      Replaces /nw${envir} in order to point to files in exec, fix and ush
      directories moved from horizontal to vertical directory structure.
    - Exports new environment variable $HOMEobsproc_network which points to
      directory path for network-specific dump subdirectories under version
      control (in production this is normally
      /nwprod/obsproc_global.$obsproc_global_ver).  Replaces /nw${envir} in
      order to point to files in parm directory moved from horizontal to
      vertical directory structure.
    - Exports new environment variables $HOMEobsproc_shared_bufr_dumplist and
      $FIXobsproc_shared_bufr_dumplist which point to directory path for
      bufr_dumplist fixed file under version control (in production the latter
      is normally
   /nwprod/obsproc_shared/bufr_dumplist.$obsproc_shared_bufr_dumplist_ver/fix).
      Replaces /nw${envir}/fix from old horizontal directory structure.
   JCDAS_DUMP:
    - Uses "cp -p" rather than "cp when copying files to /com/arkv directory
      in order to preserve group ownership as "rstprod" for restricted files.

 Parm file changes:
   prepobs_prepssmi.cdas.parm, prepobs_prepssmi.gdas.parm,
   prepobs_prepssmi.gfs.parm:
    - No changes to contents.

 Model script changes:
   exglobal_dump.sh.ecf:
    - $USHobsproc_dump replaces $utilscript as the environment variable
      representing the directory path to the ush script check_tanks.sh.
    - Modified to now dump OSCAT scatterometer data in "oscatw" using a time
      window of -3.00 to +2.99 hours relative to cycle time for the CDAS, GDAS
      and GFS.
      BENEFIT: GFS/GDAS parallel runs are now looking at these data.
    - Modified to dump GOES IR, water vapor, and visible satellite-derived
      winds using a time window of -1.00 to -0.01 hours relative to cycle time
      for the CDAS, GDAS and GFS rather than -1.50 to +1.50 hours relative to
      cycle time.
      BENEFIT: NESDIS is changing the frequency of their GOES satellite
               derived winds from 3-hourly to 1-hourly. The change to
               exglobal_dump.sh.ecf will allow one complete wind set to
               continue to be dumped and assimilated (not changing the time
               window would triple the number of winds dumped and assimilated,
               all over the same locations). This change will also allow winds
               closer to cycle time to now be assimilated.
    - Modified to skip the dumping of NeXRaD VAD winds from level 2 decoder
      (type 002, subtype 017) as part of the "vadwnd" dump ["vadwnd" will still
      contain only NeXRaD VAD winds from radar coded message (type 002, subtype
      008) for now].
      BENEFIT: Although the NeXRaD VAD winds from level 2 decoder may not yet
               otherwise even be included in the "vadwnd" dump, this will
               happen in the FY14Q2 NAM bundle - the Global GSI cannot handle
               these at this time.
    - Modified to turn off the check for the existence of "wndsat" data tanks
      and thus any attempt to dump these data..
      BENEFIT: The WindSat scatterometer data ingest feed is currently broken
               as these data are now being produced under a new format that
               NCEP does not recognize.  As we transition to the new feed,
               we do not want to inadvertently dump these data in operations.

 ECF script changes:
   cdas/dump/jcdas_dump.ecf, gdas/dump/jgdas_dump.ecf, gfs/dump/jgfs_dump.ecf:
    - Obtains versions for obsproc_global ($obsproc_global_ver),
      obsproc_dump ($obsproc_dump_ver) and obsproc_shared/bufr_dumplist
      (obsproc_shared_bufr_dumplist_ver) from obsproc_global.ver file - all
      are now exported for use by JOB scripts JCDAS_DUMP, JGDAS_DUMP, and
      JGFS_DUMP.
    - Generalized to handle all cycles.


 Output changes:
 ---------------
   Jobs JCDAS_DUMP, JGDAS_DUMP, JGFS_DUMP
    - creates new dump files
      /com/$NET/prod/$RUN.$PDY/$model.t${cyc}z.oscatw.tm00.bufr_d ,
      where cyc= 00, 06, 12, 18; NET= cdas and model= cdas for RUN= cdas;
      NET= gfs and model= gdas1 for RUN= gdas; and NET= gfs and model= gfs for
      RUN= gfs
    - "ascatw" dump files will now contain scatterometer winds from METOP-B (as
      well as from METOP-A from before)
   Job JCDAS_DUMP:
    - users in the "rstprod" group will now be able to read restricted dump
      files in the /com/arkv directory.
  

 Compute Resource Information:
 -----------------------------
   Jobs JCDAS_DUMP, JGDAS_DUMP, JGFS_DUMP
    - the "ascatw" dump files will now be approximately twice as large
      since they will now include METOP-B (as well as METOP-A from before)
         CDAS: ascatw increases from ~ 17 MBytes/day to ~ 34 MBytes/day
         GDAS: ascatw increases from ~ 17 MBytes/day to ~ 34 MBytes/day
         GFS:  ascatw increases from ~ 14 MBytes/day to ~ 28 MBytes/day
    - disk space required per day for all new output dump files is:
        cdas: oscatw   ~  88 MBytes
        gdas: oscatw   ~  87 MBytes
        gfs:  oscatw   ~  66 MBytes
        TOTAL:       ~   241 MBytes
    - 3 second increase in wallclock run time
    - uses code from obsproc_dump.v3.0.0
    - uses code from obsproc_shared/bufr_dumplist.v1.0.0
    - no other changes


 Preimplementation Testing Requirements:
 ---------------------------------------
   - Test production jobs JCDAS_DUMP, JGDAS_DUMP, JGFS_DUMP.
   - This is part of the parallel-production test of the hourly GOES satellite
     winds.


 Dissemination:
 --------------
   - The main user of this output is the CDAS, GDAS and GFS models.
   - No change in dissemination.
   - No change in archival on HPSS.


 Special Instructions:
 ---------------------
   This must be implemented at EXACTLY the same time as NESDIS' promotion of
   the hourly GOES satellite winds to their production server, and
   simultaneously with the implementations of obsproc_dump_monitor.v1.0.0,
   obsproc_dump.v3.0.0, obsproc_nam.v1.0.0, obsproc_rap.v1.0.0,
   obsproc_rtma.v1.1.0, obsproc_utma.v1.1.0, obsproc_satingest.v2.0.0,
   obsproc_shared/bufr_cword.v1.0.0 and obsproc_shared/bufr_dumplist.v1.0.0.

   Please copy new file
   /meso/save/Dennis.Keyser/HOME/versions/obsproc_global.ver to
   /nwprod/versions.

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
